{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/paperspace/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Merge, Dropout, concatenate, Dense, BatchNormalization, Lambda, TimeDistributed, Dot, dot\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from os.path import expanduser, exists\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperdash import monitor_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>Method to find separation of slits using fresn...</td>\n",
       "      <td>What are some of the things technicians can te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>What are the laws to change your status from a...</td>\n",
       "      <td>What are the laws to change your status from a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>What would a Trump presidency mean for current...</td>\n",
       "      <td>How will a Trump presidency affect the student...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>What does manipulation mean?</td>\n",
       "      <td>What does manipulation means?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>Why do girls want to be friends with the guy t...</td>\n",
       "      <td>How do guys feel after rejecting a girl?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>Why are so many Quora users posting questions ...</td>\n",
       "      <td>Why do people ask Quora questions which can be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>Which is the best digital marketing institutio...</td>\n",
       "      <td>Which is the best digital marketing institute ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>Why do rockets look white?</td>\n",
       "      <td>Why are rockets and boosters painted white?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>What's causing someone to be jealous?</td>\n",
       "      <td>What can I do to avoid being jealous of someone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>What are the questions should not ask on Quora?</td>\n",
       "      <td>Which question should I ask on Quora?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>How much is 30 kV in HP?</td>\n",
       "      <td>Where can I find a conversion chart for CC to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>What does it mean that every time I look at th...</td>\n",
       "      <td>How many times a day do a clock’s hands overlap?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>What are some tips on making it through the jo...</td>\n",
       "      <td>What are some tips on making it through the jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>What is web application?</td>\n",
       "      <td>What is the web application framework?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>Does society place too much importance on sports?</td>\n",
       "      <td>How do sports contribute to the society?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>What is best way to make money online?</td>\n",
       "      <td>What is best way to ask for money online?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>How should I prepare for CA final law?</td>\n",
       "      <td>How one should know that he/she completely pre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363831</th>\n",
       "      <td>363831</td>\n",
       "      <td>493863</td>\n",
       "      <td>292158</td>\n",
       "      <td>How do I analyse stocks for long term investment?</td>\n",
       "      <td>What are the best ways to find undervalued sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363832</th>\n",
       "      <td>363832</td>\n",
       "      <td>204870</td>\n",
       "      <td>493864</td>\n",
       "      <td>What are the natural predators of iguanas?</td>\n",
       "      <td>What are the natural predators of hamsters?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363833</th>\n",
       "      <td>363833</td>\n",
       "      <td>493865</td>\n",
       "      <td>37399</td>\n",
       "      <td>What universities does US Bank recruit new gra...</td>\n",
       "      <td>What universities does Commerce Bank recruit n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363834</th>\n",
       "      <td>363834</td>\n",
       "      <td>11212</td>\n",
       "      <td>23109</td>\n",
       "      <td>Why is Saltwater taffy candy imported in The B...</td>\n",
       "      <td>Why is Saltwater taffy candy imported in Germany?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363835</th>\n",
       "      <td>363835</td>\n",
       "      <td>493866</td>\n",
       "      <td>493867</td>\n",
       "      <td>Can I add my Paytm wallet money to my bank acc...</td>\n",
       "      <td>Can I get my certificates back from engineerin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363836</th>\n",
       "      <td>363836</td>\n",
       "      <td>269182</td>\n",
       "      <td>261936</td>\n",
       "      <td>How do underwater tunnels get built?</td>\n",
       "      <td>How are underwater tunnels constructed?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363837</th>\n",
       "      <td>363837</td>\n",
       "      <td>193292</td>\n",
       "      <td>112665</td>\n",
       "      <td>Where can I get very reliable furniture remova...</td>\n",
       "      <td>Where can I get most reliable moving services ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363838</th>\n",
       "      <td>363838</td>\n",
       "      <td>19375</td>\n",
       "      <td>32982</td>\n",
       "      <td>When will Apple release the new MacBook Pro in...</td>\n",
       "      <td>Are there any rumours about when a new MacBook...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363839</th>\n",
       "      <td>363839</td>\n",
       "      <td>83745</td>\n",
       "      <td>27805</td>\n",
       "      <td>Why are games so much more expensive on consol...</td>\n",
       "      <td>Why are console games more expensive than PC v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363840</th>\n",
       "      <td>363840</td>\n",
       "      <td>1699</td>\n",
       "      <td>10639</td>\n",
       "      <td>What were the main and most important politica...</td>\n",
       "      <td>What were the causes of World War I?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363841</th>\n",
       "      <td>363841</td>\n",
       "      <td>493868</td>\n",
       "      <td>493869</td>\n",
       "      <td>What are the major public events for next five...</td>\n",
       "      <td>What is your wish for next year?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363842</th>\n",
       "      <td>363842</td>\n",
       "      <td>493870</td>\n",
       "      <td>493871</td>\n",
       "      <td>How do you rate my drawing skills?</td>\n",
       "      <td>How would you rate my drawing skills?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363843</th>\n",
       "      <td>363843</td>\n",
       "      <td>493872</td>\n",
       "      <td>169857</td>\n",
       "      <td>What type of screen protection is there in the...</td>\n",
       "      <td>Should I buy Xiaomi Redmi Note 3? Why?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363844</th>\n",
       "      <td>363844</td>\n",
       "      <td>326778</td>\n",
       "      <td>493873</td>\n",
       "      <td>What are the most privileged and best paid job...</td>\n",
       "      <td>Where does Cyclohexane come from?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363845</th>\n",
       "      <td>363845</td>\n",
       "      <td>493874</td>\n",
       "      <td>493875</td>\n",
       "      <td>How can I get a job in HR without an MBA?</td>\n",
       "      <td>How do I become an HR for a company without an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363846</th>\n",
       "      <td>363846</td>\n",
       "      <td>52188</td>\n",
       "      <td>232219</td>\n",
       "      <td>If somebody adds you on Snapchat, why can't yo...</td>\n",
       "      <td>How do you add friends in Snapchat?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363847</th>\n",
       "      <td>363847</td>\n",
       "      <td>57096</td>\n",
       "      <td>493876</td>\n",
       "      <td>How does Apache Ambari vary from zookeeper?</td>\n",
       "      <td>What is Apache Zookeeper used for?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363848</th>\n",
       "      <td>363848</td>\n",
       "      <td>493877</td>\n",
       "      <td>493878</td>\n",
       "      <td>What African country is the most successful to...</td>\n",
       "      <td>What are the most prosperous African nations?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363849</th>\n",
       "      <td>363849</td>\n",
       "      <td>493879</td>\n",
       "      <td>493880</td>\n",
       "      <td>Where can I buy herbal cigarettes in india?</td>\n",
       "      <td>Which is the best natural herbal mehendi cone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363850</th>\n",
       "      <td>363850</td>\n",
       "      <td>493881</td>\n",
       "      <td>493882</td>\n",
       "      <td>Which is correct: \"No items selected\" or \"No i...</td>\n",
       "      <td>Can we select some adjective words for our sch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363851</th>\n",
       "      <td>363851</td>\n",
       "      <td>493883</td>\n",
       "      <td>493884</td>\n",
       "      <td>What's the quality of a dailyobject phone case...</td>\n",
       "      <td>How should I look for an engineering job in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363852</th>\n",
       "      <td>363852</td>\n",
       "      <td>194667</td>\n",
       "      <td>493885</td>\n",
       "      <td>How do I study stochastic partial differential...</td>\n",
       "      <td>How do I solve this partial differentiation eq...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363853</th>\n",
       "      <td>363853</td>\n",
       "      <td>102341</td>\n",
       "      <td>153999</td>\n",
       "      <td>How do I know if somebody likes me?</td>\n",
       "      <td>How would I know if she likes me?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363854</th>\n",
       "      <td>363854</td>\n",
       "      <td>62089</td>\n",
       "      <td>197365</td>\n",
       "      <td>What is the advice you would give your 16-year...</td>\n",
       "      <td>What advice would you give to your 18-year-old...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363855</th>\n",
       "      <td>363855</td>\n",
       "      <td>375894</td>\n",
       "      <td>85486</td>\n",
       "      <td>How can you determine the chemical formula for...</td>\n",
       "      <td>How can you determine the chemical formula for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363856</th>\n",
       "      <td>363856</td>\n",
       "      <td>493886</td>\n",
       "      <td>104093</td>\n",
       "      <td>Is there anyone from Mumbai preparing for UPSC?</td>\n",
       "      <td>Union Public Service Commission (India): What ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363857</th>\n",
       "      <td>363857</td>\n",
       "      <td>493887</td>\n",
       "      <td>493888</td>\n",
       "      <td>Why is there a road named Aurangzeb in Delhi?</td>\n",
       "      <td>How do you justify naming a road after Aurangzeb?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363858</th>\n",
       "      <td>363858</td>\n",
       "      <td>14695</td>\n",
       "      <td>44317</td>\n",
       "      <td>How can I lose weight effectively?</td>\n",
       "      <td>How do I lose weight without stopping?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363859</th>\n",
       "      <td>363859</td>\n",
       "      <td>12342</td>\n",
       "      <td>493889</td>\n",
       "      <td>Why are old TV shows square shaped on modern s...</td>\n",
       "      <td>Why are computer and TV screens typically squa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363860</th>\n",
       "      <td>363860</td>\n",
       "      <td>5902</td>\n",
       "      <td>17589</td>\n",
       "      <td>What are the safety precautions on handling sh...</td>\n",
       "      <td>What are the safety precautions on handling sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363861 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0            0       1       2   \n",
       "1            1       3       4   \n",
       "2            2       5       6   \n",
       "3            3       7       8   \n",
       "4            4       9      10   \n",
       "5            5      11      12   \n",
       "6            6      13      14   \n",
       "7            7      15      16   \n",
       "8            8      17      18   \n",
       "9            9      19      20   \n",
       "10          10      21      22   \n",
       "11          11      23      24   \n",
       "12          12      25      26   \n",
       "13          13      27      28   \n",
       "14          14      29      30   \n",
       "15          15      31      32   \n",
       "16          16      33      34   \n",
       "17          17      35      36   \n",
       "18          18      37      38   \n",
       "19          19      39      40   \n",
       "20          20      41      42   \n",
       "21          21      43      44   \n",
       "22          22      45      46   \n",
       "23          23      47      48   \n",
       "24          24      49      50   \n",
       "25          25      51      52   \n",
       "26          26      53      54   \n",
       "27          27      55      56   \n",
       "28          28      57      58   \n",
       "29          29      59      60   \n",
       "...        ...     ...     ...   \n",
       "363831  363831  493863  292158   \n",
       "363832  363832  204870  493864   \n",
       "363833  363833  493865   37399   \n",
       "363834  363834   11212   23109   \n",
       "363835  363835  493866  493867   \n",
       "363836  363836  269182  261936   \n",
       "363837  363837  193292  112665   \n",
       "363838  363838   19375   32982   \n",
       "363839  363839   83745   27805   \n",
       "363840  363840    1699   10639   \n",
       "363841  363841  493868  493869   \n",
       "363842  363842  493870  493871   \n",
       "363843  363843  493872  169857   \n",
       "363844  363844  326778  493873   \n",
       "363845  363845  493874  493875   \n",
       "363846  363846   52188  232219   \n",
       "363847  363847   57096  493876   \n",
       "363848  363848  493877  493878   \n",
       "363849  363849  493879  493880   \n",
       "363850  363850  493881  493882   \n",
       "363851  363851  493883  493884   \n",
       "363852  363852  194667  493885   \n",
       "363853  363853  102341  153999   \n",
       "363854  363854   62089  197365   \n",
       "363855  363855  375894   85486   \n",
       "363856  363856  493886  104093   \n",
       "363857  363857  493887  493888   \n",
       "363858  363858   14695   44317   \n",
       "363859  363859   12342  493889   \n",
       "363860  363860    5902   17589   \n",
       "\n",
       "                                                question1  \\\n",
       "0       What is the step by step guide to invest in sh...   \n",
       "1       What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2       How can I increase the speed of my internet co...   \n",
       "3       Why am I mentally very lonely? How can I solve...   \n",
       "4       Which one dissolve in water quikly sugar, salt...   \n",
       "5       Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                     Should I buy tiago?   \n",
       "7                          How can I be a good geologist?   \n",
       "8                         When do you use シ instead of し?   \n",
       "9       Motorola (company): Can I hack my Charter Moto...   \n",
       "10      Method to find separation of slits using fresn...   \n",
       "11            How do I read and find my YouTube comments?   \n",
       "12                   What can make Physics easy to learn?   \n",
       "13            What was your first sexual experience like?   \n",
       "14      What are the laws to change your status from a...   \n",
       "15      What would a Trump presidency mean for current...   \n",
       "16                           What does manipulation mean?   \n",
       "17      Why do girls want to be friends with the guy t...   \n",
       "18      Why are so many Quora users posting questions ...   \n",
       "19      Which is the best digital marketing institutio...   \n",
       "20                             Why do rockets look white?   \n",
       "21                  What's causing someone to be jealous?   \n",
       "22        What are the questions should not ask on Quora?   \n",
       "23                               How much is 30 kV in HP?   \n",
       "24      What does it mean that every time I look at th...   \n",
       "25      What are some tips on making it through the jo...   \n",
       "26                               What is web application?   \n",
       "27      Does society place too much importance on sports?   \n",
       "28                 What is best way to make money online?   \n",
       "29                 How should I prepare for CA final law?   \n",
       "...                                                   ...   \n",
       "363831  How do I analyse stocks for long term investment?   \n",
       "363832         What are the natural predators of iguanas?   \n",
       "363833  What universities does US Bank recruit new gra...   \n",
       "363834  Why is Saltwater taffy candy imported in The B...   \n",
       "363835  Can I add my Paytm wallet money to my bank acc...   \n",
       "363836               How do underwater tunnels get built?   \n",
       "363837  Where can I get very reliable furniture remova...   \n",
       "363838  When will Apple release the new MacBook Pro in...   \n",
       "363839  Why are games so much more expensive on consol...   \n",
       "363840  What were the main and most important politica...   \n",
       "363841  What are the major public events for next five...   \n",
       "363842                 How do you rate my drawing skills?   \n",
       "363843  What type of screen protection is there in the...   \n",
       "363844  What are the most privileged and best paid job...   \n",
       "363845          How can I get a job in HR without an MBA?   \n",
       "363846  If somebody adds you on Snapchat, why can't yo...   \n",
       "363847        How does Apache Ambari vary from zookeeper?   \n",
       "363848  What African country is the most successful to...   \n",
       "363849        Where can I buy herbal cigarettes in india?   \n",
       "363850  Which is correct: \"No items selected\" or \"No i...   \n",
       "363851  What's the quality of a dailyobject phone case...   \n",
       "363852  How do I study stochastic partial differential...   \n",
       "363853                How do I know if somebody likes me?   \n",
       "363854  What is the advice you would give your 16-year...   \n",
       "363855  How can you determine the chemical formula for...   \n",
       "363856    Is there anyone from Mumbai preparing for UPSC?   \n",
       "363857      Why is there a road named Aurangzeb in Delhi?   \n",
       "363858                 How can I lose weight effectively?   \n",
       "363859  Why are old TV shows square shaped on modern s...   \n",
       "363860  What are the safety precautions on handling sh...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0       What is the step by step guide to invest in sh...             0  \n",
       "1       What would happen if the Indian government sto...             0  \n",
       "2       How can Internet speed be increased by hacking...             0  \n",
       "3       Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4                 Which fish would survive in salt water?             0  \n",
       "5       I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6       What keeps childern active and far from phone ...             0  \n",
       "7               What should I do to be a great geologist?             1  \n",
       "8                   When do you use \"&\" instead of \"and\"?             0  \n",
       "9       How do I hack Motorola DCX3400 for free internet?             0  \n",
       "10      What are some of the things technicians can te...             0  \n",
       "11                 How can I see all my Youtube comments?             1  \n",
       "12                How can you make physics easy to learn?             1  \n",
       "13                 What was your first sexual experience?             1  \n",
       "14      What are the laws to change your status from a...             0  \n",
       "15      How will a Trump presidency affect the student...             1  \n",
       "16                          What does manipulation means?             1  \n",
       "17               How do guys feel after rejecting a girl?             0  \n",
       "18      Why do people ask Quora questions which can be...             1  \n",
       "19      Which is the best digital marketing institute ...             0  \n",
       "20            Why are rockets and boosters painted white?             1  \n",
       "21       What can I do to avoid being jealous of someone?             0  \n",
       "22                  Which question should I ask on Quora?             0  \n",
       "23      Where can I find a conversion chart for CC to ...             0  \n",
       "24       How many times a day do a clock’s hands overlap?             0  \n",
       "25      What are some tips on making it through the jo...             0  \n",
       "26                 What is the web application framework?             0  \n",
       "27               How do sports contribute to the society?             0  \n",
       "28              What is best way to ask for money online?             0  \n",
       "29      How one should know that he/she completely pre...             1  \n",
       "...                                                   ...           ...  \n",
       "363831  What are the best ways to find undervalued sto...             0  \n",
       "363832        What are the natural predators of hamsters?             0  \n",
       "363833  What universities does Commerce Bank recruit n...             0  \n",
       "363834  Why is Saltwater taffy candy imported in Germany?             1  \n",
       "363835  Can I get my certificates back from engineerin...             0  \n",
       "363836            How are underwater tunnels constructed?             1  \n",
       "363837  Where can I get most reliable moving services ...             1  \n",
       "363838  Are there any rumours about when a new MacBook...             1  \n",
       "363839  Why are console games more expensive than PC v...             1  \n",
       "363840               What were the causes of World War I?             1  \n",
       "363841                   What is your wish for next year?             0  \n",
       "363842              How would you rate my drawing skills?             0  \n",
       "363843             Should I buy Xiaomi Redmi Note 3? Why?             0  \n",
       "363844                  Where does Cyclohexane come from?             0  \n",
       "363845  How do I become an HR for a company without an...             1  \n",
       "363846                How do you add friends in Snapchat?             0  \n",
       "363847                 What is Apache Zookeeper used for?             0  \n",
       "363848      What are the most prosperous African nations?             1  \n",
       "363849  Which is the best natural herbal mehendi cone ...             0  \n",
       "363850  Can we select some adjective words for our sch...             0  \n",
       "363851  How should I look for an engineering job in th...             0  \n",
       "363852  How do I solve this partial differentiation eq...             0  \n",
       "363853                  How would I know if she likes me?             0  \n",
       "363854  What advice would you give to your 18-year-old...             0  \n",
       "363855  How can you determine the chemical formula for...             0  \n",
       "363856  Union Public Service Commission (India): What ...             0  \n",
       "363857  How do you justify naming a road after Aurangzeb?             0  \n",
       "363858             How do I lose weight without stopping?             1  \n",
       "363859  Why are computer and TV screens typically squa...             0  \n",
       "363860  What are the safety precautions on handling sh...             1  \n",
       "\n",
       "[363861 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_csv('train.csv')\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN\n",
    "\n",
    "We need to find if two questions are similar. In face recognition, we use siamese networks to solve similar problem but for faces. So, we'll try using siamese networks here - only we'll use LSTMs instead of CNNs since LSTMs are suited for sequences.\n",
    "\n",
    "0. Check the data. If we have enough q-ids for which we have duplicates available, then we could train the whole thing via triplet loss. If so, follow plan a, else plan b.\n",
    "\n",
    "1. Pre-processing\n",
    "    - Remove questionmarks throughout\n",
    "    - Remove stop-words (Save one which keeps stop-words as well)\n",
    "\n",
    "2. Convert to vectors\n",
    "3. Divide in 70/30 split. (Also try 80/20 split)\n",
    "4. Pass through siamese LSTMs\n",
    "\n",
    "**Plan a**\n",
    "5. Use squared distance\n",
    "\n",
    "**Plan b**\n",
    "5. Use triplet loss (find all those q-ids which have duplicates available for them first. See if it makes sense to use triplet loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>363861.000000</td>\n",
       "      <td>363861.000000</td>\n",
       "      <td>363861.000000</td>\n",
       "      <td>363861.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>181930.000000</td>\n",
       "      <td>201899.281913</td>\n",
       "      <td>204884.863951</td>\n",
       "      <td>0.371502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>105037.767486</td>\n",
       "      <td>144924.825062</td>\n",
       "      <td>146663.968132</td>\n",
       "      <td>0.483207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>90965.000000</td>\n",
       "      <td>70779.000000</td>\n",
       "      <td>70942.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>181930.000000</td>\n",
       "      <td>179999.000000</td>\n",
       "      <td>184182.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>272895.000000</td>\n",
       "      <td>321295.000000</td>\n",
       "      <td>327744.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>363860.000000</td>\n",
       "      <td>493887.000000</td>\n",
       "      <td>493889.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           qid1           qid2   is_duplicate\n",
       "count  363861.000000  363861.000000  363861.000000  363861.000000\n",
       "mean   181930.000000  201899.281913  204884.863951       0.371502\n",
       "std    105037.767486  144924.825062  146663.968132       0.483207\n",
       "min         0.000000       1.000000       2.000000       0.000000\n",
       "25%     90965.000000   70779.000000   70942.000000       0.000000\n",
       "50%    181930.000000  179999.000000  184182.000000       0.000000\n",
       "75%    272895.000000  321295.000000  327744.000000       1.000000\n",
       "max    363860.000000  493887.000000  493889.000000       1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_copy = train_dataset.copy()\n",
    "train_df_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>135175.000000</td>\n",
       "      <td>135175.000000</td>\n",
       "      <td>135175.000000</td>\n",
       "      <td>135175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>181735.176741</td>\n",
       "      <td>156901.917507</td>\n",
       "      <td>157247.986292</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>105058.800004</td>\n",
       "      <td>137618.655600</td>\n",
       "      <td>137577.456205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>90843.000000</td>\n",
       "      <td>39315.000000</td>\n",
       "      <td>39697.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>181718.000000</td>\n",
       "      <td>113964.000000</td>\n",
       "      <td>113489.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>272849.500000</td>\n",
       "      <td>250886.000000</td>\n",
       "      <td>251945.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>363860.000000</td>\n",
       "      <td>493877.000000</td>\n",
       "      <td>493878.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id           qid1           qid2  is_duplicate\n",
       "count  135175.000000  135175.000000  135175.000000      135175.0\n",
       "mean   181735.176741  156901.917507  157247.986292           1.0\n",
       "std    105058.800004  137618.655600  137577.456205           0.0\n",
       "min         5.000000      11.000000      12.000000           1.0\n",
       "25%     90843.000000   39315.000000   39697.000000           1.0\n",
       "50%    181718.000000  113964.000000  113489.000000           1.0\n",
       "75%    272849.500000  250886.000000  251945.000000           1.0\n",
       "max    363860.000000  493877.000000  493878.000000           1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_copy[train_df_copy['is_duplicate'] > 0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number of unique questions whose duplicates we have**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80105"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_copy[train_df_copy['is_duplicate'] > 0]['qid1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number of unique questions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266358"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_copy['qid1'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision** : We could go this path and use triplet loss, however, triplet loss uses A(anchor), P(positive) and N(negative) triplet and it's very important to find a N which is closer to A but still not a duplicate. For us to find those pairs would be a time-taking exercise which I could try to do after basic model, perhaps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a list of all vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_set = set(train_df['question1'].unique())\n",
    "q2_set = set(train_df['question2'].unique())\n",
    "all_ques_list = q1_set | q2_set\n",
    "len(all_ques_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india? : What is the step by step guide to invest in share market? : 0\n"
     ]
    }
   ],
   "source": [
    "q1_list = train_df['question1'].tolist()\n",
    "q1_list = [str(ques) for ques in q1_list]\n",
    "q2_list = train_df['question2'].tolist()\n",
    "q2_list = [str(ques) for ques in q2_list]\n",
    "is_duplicate_list = train_df['is_duplicate'].tolist()\n",
    "\n",
    "print(q1_list[0],\":\",q2_list[0],\":\",is_duplicate_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 91013\n"
     ]
    }
   ],
   "source": [
    "all_questions_list = q1_list + q2_list\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(all_questions_list)\n",
    "\n",
    "q1_word_seq = tokenizer.texts_to_sequences(q1_list)\n",
    "q2_word_seq = tokenizer.texts_to_sequences(q2_list)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer word index we've gotten for later\n",
    "\n",
    "dictionary = word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing glove.840B.300d.txt\n",
      "Word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DOWNLOAD_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "\n",
    "if not exists(expanduser('~/.keras/datasets/glove.840B.300d.zip')):\n",
    "    zipfile = ZipFile(get_file('glove.840B.300d.zip', GLOVE_DOWNLOAD_URL))\n",
    "    zipfile.extract('glove.840B.300d.txt', path=expanduser('~/.keras/datasets/'))\n",
    "    \n",
    "print(\"Processing\", 'glove.840B.300d.txt')\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(expanduser('~/.keras/datasets/glove.840B.300d.txt'), encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 27053\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 1, 1245, 57, 1245, 2546, 7, 577, 8, 772, 379, 8, 35],\n",
       " [2, 3, 1, 562, 10, 13509, 14684, 5, 21439, 4449],\n",
       " [4, 13, 5, 219, 1, 439, 10, 17, 364, 1848, 205, 146, 6, 2836],\n",
       " [16, 72, 5, 2693, 309, 2764, 4, 13, 5, 661, 19],\n",
       " [23, 49, 7202, 8, 233, 33752, 1906, 2077, 10473, 12, 1927, 10671, 6462]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_word_seq[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 0\n",
    "for ques in q1_word_seq:\n",
    "    if(len(ques) > max_seq_length):\n",
    "        max_seq_length = len(ques)\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of question1 data tensor: (363861, 130)\n",
      "Shape of question2 data tensor: (363861, 130)\n",
      "Shape of label tensor: (363861,)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 130\n",
    "\n",
    "q1_data = pad_sequences(q1_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(is_duplicate_list, dtype=int)\n",
    "print('Shape of question1 data tensor:', q1_data.shape)\n",
    "print('Shape of question2 data tensor:', q2_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     2,\n",
       "            3,     1,  1245,    57,  1245,  2546,     7,   577,     8,\n",
       "          772,   379,     8,    35],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     2,     3,     1,   562,    10, 13509,\n",
       "        14684,     5, 21439,  4449],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     4,\n",
       "           13,     5,   219,     1,   439,    10,    17,   364,  1848,\n",
       "          205,   146,     6,  2836],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,    16,    72,     5,  2693,   309,  2764,     4,\n",
       "           13,     5,   661,    19],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           23,    49,  7202,     8,   233, 33752,  1906,  2077, 10473,\n",
       "           12,  1927, 10671,  6462]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363861, 2, 130)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291088, 130)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]\n",
    "Q1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   229,  7037,   229],\n",
       "       [    0,     0,     0, ...,    10,  1558,   313],\n",
       "       [    0,     0,     0, ...,     6,  6348,   333],\n",
       "       ..., \n",
       "       [    0,     0,     0, ..., 20350,     8,  1307],\n",
       "       [    0,     0,     0, ...,     7,   114, 10766],\n",
       "       [    0,     0,     0, ..., 36739,    12, 23563]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291088, 130)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 130)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 130)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 130, 300)      27304200    input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            70200       embedding_1[0][0]                \n",
      "                                                                   embedding_1[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 50)            0           lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           5100        dropout_1[0][0]                  \n",
      "                                                                   dropout_1[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 200)           0           dense_1[0][0]                    \n",
      "                                                                   dense_1[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             201         concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 27,379,701\n",
      "Trainable params: 75,501\n",
      "Non-trainable params: 27,304,200\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_HIDDEN_UNITS_LAYER1 = 50\n",
    "NUM_HIDDEN_UNITS_LAYER2 = 100\n",
    "\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "embedding_layer = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)\n",
    "\n",
    "q1 = embedding_layer(question1)\n",
    "q2 = embedding_layer(question2)\n",
    "\n",
    "lstm_first = LSTM(NUM_HIDDEN_UNITS_LAYER1, return_sequences=False)\n",
    "\n",
    "q1 = lstm_first(q1)\n",
    "q2 = lstm_first(q2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "\n",
    "q1 = dropout_layer(q1)\n",
    "q2 = dropout_layer(q2)\n",
    "\n",
    "dense = Dense(100, activation='relu')\n",
    "dropout_two = Dropout(0.2)\n",
    "bn_one = BatchNormalization()\n",
    "\n",
    "q1 = dense(q1)\n",
    "# q1 = dropout_two(q1)\n",
    "# q1 = bn_one(q1)\n",
    "q2 = dense(q2)\n",
    "# q2 = dropout_two(q2)\n",
    "# q2 = bn_one(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2018-06-03 10:16:32.628602\n",
      "Train on 291088 samples, validate on 72773 samples\n",
      "Epoch 1/20\n",
      "291088/291088 [==============================] - 243s - loss: 0.5549 - acc: 0.7164 - val_loss: 0.5350 - val_acc: 0.7329\n",
      "Epoch 2/20\n",
      "291088/291088 [==============================] - 237s - loss: 0.5151 - acc: 0.7468 - val_loss: 0.5100 - val_acc: 0.7532\n",
      "Epoch 3/20\n",
      "291088/291088 [==============================] - 238s - loss: 0.4986 - acc: 0.7582 - val_loss: 0.5003 - val_acc: 0.7610\n",
      "Epoch 4/20\n",
      "291088/291088 [==============================] - 236s - loss: 0.4866 - acc: 0.7659 - val_loss: 0.4924 - val_acc: 0.7643\n",
      "Epoch 5/20\n",
      "291088/291088 [==============================] - 235s - loss: 0.4769 - acc: 0.7721 - val_loss: 0.4883 - val_acc: 0.7686\n",
      "Epoch 6/20\n",
      "291088/291088 [==============================] - 232s - loss: 0.4675 - acc: 0.7776 - val_loss: 0.4853 - val_acc: 0.7695\n",
      "Epoch 7/20\n",
      "291088/291088 [==============================] - 232s - loss: 0.4607 - acc: 0.7825 - val_loss: 0.4816 - val_acc: 0.7723\n",
      "Epoch 8/20\n",
      "291088/291088 [==============================] - 231s - loss: 0.4535 - acc: 0.7858 - val_loss: 0.4815 - val_acc: 0.7732\n",
      "Epoch 9/20\n",
      "291088/291088 [==============================] - 231s - loss: 0.4459 - acc: 0.7908 - val_loss: 0.4856 - val_acc: 0.7690\n",
      "Epoch 10/20\n",
      "291088/291088 [==============================] - 231s - loss: 0.4395 - acc: 0.7948 - val_loss: 0.4806 - val_acc: 0.7743\n",
      "Epoch 11/20\n",
      "291088/291088 [==============================] - 230s - loss: 0.4341 - acc: 0.7970 - val_loss: 0.4824 - val_acc: 0.7741\n",
      "Epoch 12/20\n",
      "291088/291088 [==============================] - 231s - loss: 0.4277 - acc: 0.8013 - val_loss: 0.4803 - val_acc: 0.7782\n",
      "Epoch 13/20\n",
      "291088/291088 [==============================] - 232s - loss: 0.4221 - acc: 0.8047 - val_loss: 0.4834 - val_acc: 0.7779\n",
      "Epoch 14/20\n",
      "291088/291088 [==============================] - 230s - loss: 0.4171 - acc: 0.8073 - val_loss: 0.4816 - val_acc: 0.7783\n",
      "Epoch 15/20\n",
      "291088/291088 [==============================] - 231s - loss: 0.4112 - acc: 0.8101 - val_loss: 0.4826 - val_acc: 0.7821\n",
      "Epoch 16/20\n",
      "291088/291088 [==============================] - 232s - loss: 0.4063 - acc: 0.8131 - val_loss: 0.4821 - val_acc: 0.7801\n",
      "Epoch 17/20\n",
      "291088/291088 [==============================] - 229s - loss: 0.4013 - acc: 0.8150 - val_loss: 0.4817 - val_acc: 0.7813\n",
      "Epoch 18/20\n",
      "291088/291088 [==============================] - 230s - loss: 0.3967 - acc: 0.8173 - val_loss: 0.4854 - val_acc: 0.7781\n",
      "Epoch 19/20\n",
      "291088/291088 [==============================] - 229s - loss: 0.3917 - acc: 0.8200 - val_loss: 0.4935 - val_acc: 0.7722\n",
      "Epoch 20/20\n",
      "291088/291088 [==============================] - 230s - loss: 0.3875 - acc: 0.8221 - val_loss: 0.4917 - val_acc: 0.7783\n",
      "Training ended at 2018-06-03 11:34:15.437467\n",
      "Minutes elapsed: 77.713477\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint('question_pairs_weights_type1_final_new.h5', monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=([Q1_test, Q2_test], y_test),\n",
    "                    verbose=1,\n",
    "                    batch_size=512,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text, dictionary):\n",
    "\twords = text_to_word_sequence(text)\n",
    "\twordIndices = []\n",
    "\tfor word in words:\n",
    "\t    if word in dictionary:\n",
    "\t        wordIndices.append(dictionary[word])\n",
    "\t    else:\n",
    "\t        print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "\treturn wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01451303]]\n"
     ]
    }
   ],
   "source": [
    "# HAPPY CASE\n",
    "question1 = \"What's r programming?\"\n",
    "question2 = \"What's in r programming?\"\n",
    "\n",
    "q1_word_seq = convert_text_to_index_array(question1,dictionary)\n",
    "q1_word_seq = [q1_word_seq]\n",
    "q2_word_seq = convert_text_to_index_array(question2,dictionary)\n",
    "q2_word_seq = [q2_word_seq]\n",
    "q1_data = pad_sequences(q1_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "pred = model.predict([q1_data,q2_data])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.56790417]]\n"
     ]
    }
   ],
   "source": [
    "question1 = \"How to learn english?\"\n",
    "question2 = \"Why can't I dance?\"\n",
    "\n",
    "q1_word_seq = convert_text_to_index_array(question1,dictionary)\n",
    "q1_word_seq = [q1_word_seq]\n",
    "q2_word_seq = convert_text_to_index_array(question2,dictionary)\n",
    "q2_word_seq = [q2_word_seq]\n",
    "q1_data = pad_sequences(q1_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(q2_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "pred = model.predict([q1_data,q2_data])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 -- that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)\n",
    "q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "q2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)\n",
    "q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_81 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_82 (InputLayer)            (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_42 (Embedding)         (None, 30, 300)       27304200    input_81[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_43 (Embedding)         (None, 30, 300)       27304200    input_82[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistribu (None, 30, 300)       90300       embedding_42[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistribu (None, 30, 300)       90300       embedding_43[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)               (None, 300)           0           time_distributed_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)               (None, 300)           0           time_distributed_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)     (None, 600)           0           lambda_45[0][0]                  \n",
      "                                                                   lambda_46[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 200)           120200      concatenate_28[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)             (None, 200)           0           dense_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 200)           800         dropout_47[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 200)           40200       batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)             (None, 200)           0           dense_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 200)           800         dropout_48[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_33 (Dense)                 (None, 200)           40200       batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)             (None, 200)           0           dense_33[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 200)           800         dropout_49[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_34 (Dense)                 (None, 200)           40200       batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)             (None, 200)           0           dense_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 200)           800         dropout_50[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_35 (Dense)                 (None, 1)             201         batch_normalization_12[0][0]     \n",
      "====================================================================================================\n",
      "Total params: 55,033,201\n",
      "Trainable params: 423,201\n",
      "Non-trainable params: 54,610,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2018-06-03 01:26:59.153656\n",
      "Train on 291088 samples, validate on 72773 samples\n",
      "Epoch 1/25\n",
      "291088/291088 [==============================] - 57s - loss: 0.5124 - acc: 0.7441 - val_loss: 0.4834 - val_acc: 0.7638\n",
      "Epoch 2/25\n",
      "291088/291088 [==============================] - 53s - loss: 0.4720 - acc: 0.7691 - val_loss: 0.4670 - val_acc: 0.7707\n",
      "Epoch 3/25\n",
      "291088/291088 [==============================] - 52s - loss: 0.4425 - acc: 0.7879 - val_loss: 0.5047 - val_acc: 0.7344\n",
      "Epoch 4/25\n",
      "291088/291088 [==============================] - 53s - loss: 0.4213 - acc: 0.8009 - val_loss: 0.4447 - val_acc: 0.7886\n",
      "Epoch 5/25\n",
      "291088/291088 [==============================] - 52s - loss: 0.4021 - acc: 0.8128 - val_loss: 0.4450 - val_acc: 0.7812\n",
      "Epoch 6/25\n",
      "291088/291088 [==============================] - 53s - loss: 0.3860 - acc: 0.8222 - val_loss: 0.4220 - val_acc: 0.8004\n",
      "Epoch 7/25\n",
      "291088/291088 [==============================] - 52s - loss: 0.3737 - acc: 0.8295 - val_loss: 0.4345 - val_acc: 0.7830\n",
      "Epoch 8/25\n",
      "291088/291088 [==============================] - 52s - loss: 0.3599 - acc: 0.8373 - val_loss: 0.4246 - val_acc: 0.7956\n",
      "Epoch 9/25\n",
      "291088/291088 [==============================] - 52s - loss: 0.3470 - acc: 0.8448 - val_loss: 0.4519 - val_acc: 0.7730\n",
      "Epoch 10/25\n",
      "291088/291088 [==============================] - 53s - loss: 0.3359 - acc: 0.8508 - val_loss: 0.4112 - val_acc: 0.8062\n",
      "Epoch 11/25\n",
      "290816/291088 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8556"
     ]
    }
   ],
   "source": [
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint('question_pairs_weights.h5', monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=25,\n",
    "                    validation_data=([Q1_test, Q2_test], y_test),\n",
    "                    verbose=1,\n",
    "                    batch_size=64,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links that helped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[An example of model for same problem but in R](https://tensorflow.rstudio.com/blog/keras-duplicate-questions-quora.html)\n",
    "    \n",
    "[Manhattan distance model with siamese network approach for same problem](https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb)\n",
    "\n",
    "[Another approach for same problem](https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/quora-question-pairs-training.ipynb)\n",
    "\n",
    "[Data pre-processing](https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/quora-question-pairs-data-prep.ipynb)\n",
    "\n",
    "[How to save tokenizer dictionary - word indices](https://gist.github.com/vgpena/b1c088f3c8b8c2c65dd8edbe0eae7023#file-makemodel-py-L27)\n",
    "\n",
    "[Intuitive explanation of word embeddings](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)\n",
    "                                                  \n",
    "[keras starter script for word embeddings](https://www.kaggle.com/sudalairajkumar/keras-starter-script-with-word-embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159039"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_ques_list\n",
    "vocabulary = set()\n",
    "\n",
    "for question in all_ques_list:\n",
    "    question = str(question)\n",
    "    question = question.lower()\n",
    "    question = re.sub(r\"\\?\", \" \", question)\n",
    "    question_words = question.split()\n",
    "    for word in question_words:\n",
    "        vocabulary.add(word)\n",
    "\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavul.g/.virtualenvs/ailearn/lib/python3.5/site-packages/keras/preprocessing/text.py:157: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91013 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(159039, 40)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=300)\n",
    "tokenizer.fit_on_texts(vocabulary)\n",
    "sequences_train = tokenizer.texts_to_sequences(vocabulary)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data_train = pad_sequences(sequences_train, maxlen=40)\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f03bed72d473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquestion2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m embedding_layer = Embedding(nb_words + 1, \n\u001b[0m\u001b[1;32m      8\u001b[0m                  \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                  \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_embedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_words' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_HIDDEN_UNITS_LAYER1 = 50\n",
    "NUM_HIDDEN_UNITS_LAYER2 = 100\n",
    "\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "embedding_layer = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)\n",
    "\n",
    "encoded_q1 = embedding_layer(question1)\n",
    "encoded_q2 = embedding_layer(question2)\n",
    "\n",
    "lstm_first = LSTM(NUM_HIDDEN_UNITS_LAYER1, return_sequences=True)\n",
    "\n",
    "lstm_output_q1 = lstm_first(encoded_q1)\n",
    "lstm_output_q2 = lstm_first(encoded_q2)\n",
    "\n",
    "dropout_layer = Dropout(0.2)\n",
    "\n",
    "dropout_q1 = dropout_layer(lstm_output_q1)\n",
    "dropout_q2 = dropout_layer(lstm_output_q2)\n",
    "\n",
    "q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM,))(dropout_q1)\n",
    "q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM,))(dropout_q2)\n",
    "\n",
    "#lstm_second = LSTM(NUM_HIDDEN_UNITS_LAYER2, return_sequences=False)\n",
    "\n",
    "#lstm_second_output_q1 = lstm_second(dropout_q1)\n",
    "#lstm_second_output_q2 = lstm_second(dropout_q2)\n",
    "\n",
    "#dropout_second_q1 = dropout_layer(lstm_second_output_q1)\n",
    "#dropout_second_q2 = dropout_layer(lstm_second_output_q2)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "#malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]), output_shape=lambda x: (x[0][0], 1))([dropout_q1, dropout_q2])\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "# is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "merged = Dense(600, activation='relu')(merged)\n",
    "#merged = Dropout(0.2)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "# merged = Dense(200, activation='relu')(merged)\n",
    "# merged = Dropout(0.2)(merged)\n",
    "# merged = BatchNormalization()(merged)\n",
    "# is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=merged)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Pack it all up into a model\n",
    "# malstm = Model([question1, question2], [malstm_distance])\n",
    "\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "# optimizer = Adadelta()\n",
    "\n",
    "# malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "# malstm_trained = malstm.fit([Q1_train, Q2_train], y_train, batch_size=128, nb_epoch=10,\n",
    "#                             validation_data=([Q1_test, Q2_test], y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
